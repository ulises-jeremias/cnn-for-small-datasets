<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>CNN for Small Datasets</title>
    <link rel="stylesheet" href="./css/reveal.css" />
    <link rel="stylesheet" href="./css/theme/league.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/atom-one-dark.css" />
    <link rel="stylesheet" href="./css/print/paper.css" type="text/css" media="print" />
    <link rel="stylesheet" href="./css/styles.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template">

<!-- .slide: style="text-align: right" data-background-image="./static/background.jpg" -->

### Redes Neuronales Convolucionales
## para Datasets pequeños

<br />
<br />

Ulises J. Cornejo Fandos
<br />
<small>
ucornejo@lidi.info.unlp.edu.ar
</small>

<div style="width: 145%; margin-top: 55px; margin-left: -20%; background-color: rgba(0,0,0,0.45)">
![](static/lidi.jpg)
<!-- .element: style="margin-right: 245px; margin-bottom: 5px; height: 145px; border: none; opacity: 0.953" -->
</div>
</script></section><section ><section data-markdown><script type="text/template">
### Presentación del Problema
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

El uso de modelos como redes neuronales recurrentes y convolucionales han mostrado mejoras en el area de detección de objetos y clasificación de imagenes.
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

Estos avances son impulsados ​​por una combinación de mejoras en tres áreas:

- Mejores conjuntos de datos <!-- .element: class="fragment" -->
- Mejores modelos <!-- .element: class="fragment" -->
- Potencia de computo <!-- .element: class="fragment" -->

<aside class="notes"><p>Si bien los dos últimos son en su mayoría independientes de un campo en particular, <strong>la disponibilidad de conjuntos de datos de calidad para un campo determinado limita la aplicación de estos nuevos avances.</strong></p>
</aside></script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

La falta de datos perjudica en:

- aprovechar al máximo los modelos actuales.
- el desarrollo de modelos precisos en reconocimiento de objetos.
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

En particular, las redes neuronales convolucionales (CNN), un tipo de red neuronal que aprovecha las capas convolucionales para aprender filtros convolucionales arbitrarios, han demostrado ser muy efectivas en la clasificación de imágenes.
</script></section><section data-markdown><script type="text/template">
En la mayoría de las aplicaciones, las redes neuronales convolucionales se entrenan utilizando miles de imágenes por clase, pero...

**¿Qué pasa si no contamos con esa cantidad de datos?**
<!-- .element: class="fragment" -->

</script></section></section><section ><section data-markdown><script type="text/template">
## Redes Neuronales Convolucionales

### (CNN o ConvNet)
<!-- .element: class="fragment" -->
</script></section><section data-markdown><script type="text/template">
Una ConvNet es una clase de redes neuronales profundas. Las CNN son versiones regularizadas de perceptrones multicapa (MLP), las cuales generalmente significan redes completamente conectadas.

![MLPvsCNN](static/MLPvsCNN.png)
<!-- .element: style="border: none" -->
</script></section><section data-markdown><script type="text/template">
### Problema de MLP

**Overfitting**
<!-- .element: class="fragment" -->
</script></section><section data-markdown><script type="text/template">
Las CNN adoptan un enfoque diferente hacia la regularización: aprovechan el patrón jerárquico en los datos y ensamblan patrones más complejos utilizando patrones más pequeños y simples.

En resumen, el tamaño de una CNN suele ser menor y la cantidad de pesos es mucho mas reducida, y por cada capa se reduce esta cantidad.
<!-- .element: class="fragment" -->

</script></section></section><section ><section data-markdown><script type="text/template">
# Modelos
</script></section><section data-markdown><script type="text/template">
Para resolver este problema vamos a analizar dos modelos:

- DenseNet <!-- .element: class="fragment" -->
- Prototypical Networks <!-- .element: class="fragment" -->
</script></section></section><section ><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

## DenseNet

- Estado del Arte en la clasificación de imagenes con CNN
- No diseñado especificamente para datasets pequeños
    - **puede manejarlos con baja tasa de error**
</script></section><section data-markdown><script type="text/template">
DenseNet funciona concatenando los mapas de características de un bloque convolucional con los mapas de características de todos los bloques convolucionales anteriores y usando este valor como entrada para el siguiente bloque convolucional.

<center>
![DenseNet](static/DenseConnectivity.png)
<!-- .element: style="width: 75%; border: none" -->
</center>
</script></section><section data-markdown><script type="text/template">
Cada bloque convolucional recibe todo el conocimiento colectivo de las capas anteriores manteniendo el estado global de la red a la que se puede acceder
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

## Ventajas

- Mejor eficiencia de parámetros
<small>No es necesario volver a aprender los mapas de características redundantes</small>

- Fortalecen la propagación de características

- Fomentan la reutilización de características
</script></section><section data-markdown><script type="text/template">
![DenseNet](static/DenseNet.png)
<!-- .element: style="border: none" -->
</script></section></section><section ><section data-markdown><script type="text/template">
## Prototypical Networks
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

### Prototypical Networks

Modelo de meta-aprendizaje para el problema de la clasificación de pocos "disparos".
</script></section><section data-markdown><script type="text/template">
Few shot learning generalmente se mide por su rendimiento en tareas de clasificación n-shot y k-way.
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

### Algoritmo

- Primero, un modelo es dado de una muestra de consulta que pertenece a una clase nueva nunca antes vista. 

- Luego, también se proporciona un conjunto de soporte, S, que consta de n ejemplos, cada uno de k diferentes clases no vistas. 

- Finalmente, el algoritmo debe determinar a qué clase de conjunto de soporte pertenecen las muestras de consulta. 
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

### Algoritmo

![](static/protonets.png)
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

Los esquemas para algunas tareas de few shot classification, como las Prototypical Nets, también pueden ser útiles para entrenar pequeños conjuntos de datos donde se conocen todas las clases.


</script></section></section><section ><section data-markdown><script type="text/template">
## Reconocimiento de forma de mano
</script></section><section data-markdown><script type="text/template">
### Conjuntos de datos

- <small>**LSA16**, 16 formas de mano del Lenguaje de Señas Argentino (LSA) para un total de 800 imágenes de tamaño 32x32.</small>

- <small>**RWTH** está compuesto por una selección de imágenes de tamaño 132x92 recortadas de videos de los intérpretes de lenguaje de señas en la estación de televisión pública alemana PHOENIX.De un total de 45 clases no quedamos con aquellas que cuentan con más de 20 ejemplos.</small>

- <small>**CIARP** contiene 6000 imágenes de tamaño 38x38 adquiridas por una cámara de un solo color. Las imágenes fueron etiquetadas manualmente y corresponden a 10 clases de gestos con las manos.</small>
</script></section><section data-markdown><script type="text/template">
### Conjuntos de datos

![datasets](static/datasets.png)
<!-- .element: style="width: 75%" -->
</script></section><section data-markdown><script type="text/template">
## Pasos
</script></section><section data-markdown><script type="text/template">
### Data Augmentation

- Flip
- Rotación (10 a 30 grados)
- Redimensionar (10% o 20% en ancho y alto)
</script></section><section data-markdown><script type="text/template">
### Resultados

![results](static/results.png)
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

### ¿Qué sigue?

- Comparar con otros datasets
<small>Entender mejor la relación entre los modelos y la complejidad del conjunto de datos</small>

- Comparar modelos previamente entrenados

- Investigar métodos que aprovechen datos no etiquetados.

- Combinar conjuntos de datos para aumentar la muestra.

- Data augmentation.</script></section></section></div>
    </div>

    <script src="./lib/js/head.min.js"></script>
    <script src="./js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './lib/js/classList.js', condition: function() { return !document.body.classList; } },
        { src: './plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './plugin/zoom-js/zoom.js', async: true },
        { src: './plugin/notes/notes.js', async: true },
        { src: './plugin/math/math.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
