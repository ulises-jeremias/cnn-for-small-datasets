<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>CNN for Small Datasets</title>
    <link rel="stylesheet" href="./css/reveal.css" />
    <link rel="stylesheet" href="./css/theme/league.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/atom-one-dark.css" />
    <link rel="stylesheet" href="./css/print/paper.css" type="text/css" media="print" />
    <link rel="stylesheet" href="./css/styles.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template">

<!-- .slide: style="text-align: right" data-background-image="./static/background.jpg" -->

### Recognizing Handshapes using
## Small Datasets

<br />
<br />

Ulises Jeremias Cornejo Fandos
<br />
<small>
ucornejo@lidi.info.unlp.edu.ar
</small>

<center style="width: 145%; margin-top: 50px; margin-left: -20%; background-color: rgba(0,0,0,0.45)">
<div class="row">
    <div class="col-1"></div>
    <div class="col-3">
        ![](static/unlp.png)
        <!-- .element: style="margin-top: 5px; height: 10rem; border: none; background-color: transparent;" -->
    </div>
    <div class="col-4">
        ![](static/informatica.png)
        <!-- .element: style="margin-bottom: 5px; height: 10rem; border: none; background-color: transparent;" -->
    </div>
    <div class="col-3">
        ![](static/lidi.png)
        <!-- .element: style="margin-bottom: 5px; margin-top: -10px; height: 13rem; border: none; background-color: transparent;" -->
    </div>
    <div class="col-1"></div>
</div>
</center>
</script></section><section ><section data-markdown><script type="text/template">
## Reconocimiento de Lenguaje de Señas
</script></section><section data-markdown><script type="text/template">
## Visión por Computador
# +
## Traducción de Lenguajes
</script></section><section data-markdown><script type="text/template">
![datasets](static/slr/intro1.png)
<!-- .element: style="width: 75%" -->
</script></section><section data-markdown><script type="text/template">
## Detección de Formas de Manos
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

El uso de modelos como redes neuronales recurrentes y convolucionales han mostrado mejoras en el area de detección de objetos y clasificación de imagenes.
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

Impulsados por mejoras en tres areas:

- Mejores conjuntos de datos
- Mejores modelos
- Potencia de cómputo

<aside class="notes"><p>Si bien los dos últimos son en su mayoría independientes de un campo en particular, <strong>la disponibilidad de conjuntos de datos de calidad para un campo determinado limita la aplicación de estos nuevos avances.</strong></p>
</aside></script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

La falta de datos perjudica en:

- aprovechar al máximo los modelos actuales.
- el desarrollo de modelos precisos en reconocimiento de objetos.
</script></section><section data-markdown><script type="text/template">
**¿Qué pasa si no contamos con una buena cantidad de datos?**

</script></section></section><section  data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

## Modelos

- **DenseNet**

    <small><em>Huang, G. et al: Densely connected convolutional networks, 2017</em></small>

- **Prototypical Networks**

    <small><em>Snell, J. et al: Prototypical networks for few-shot learning, 2017</em></small>
</script></section><section ><section data-markdown><script type="text/template">
# DenseNet
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

## DenseNet

- Estado del Arte en la clasificación de imagenes con CNN
- No diseñado especificamente para datasets pequeños
    - **puede manejarlos con baja tasa de error**
</script></section><section data-markdown><script type="text/template">
<center>
![DenseNet](static/DenseConnectivity.png)
<!-- .element: style="width: 75%; border: none" -->
</center>

<aside class="notes"><p>DenseNet funciona concatenando los mapas de características de un bloque convolucional con los mapas de características de todos los bloques convolucionales anteriores y usando este valor como entrada para el siguiente bloque convolucional.</p>
</aside></script></section><section data-markdown><script type="text/template">
![DenseNet](static/DenseNet.png)
<!-- .element: style="border: none" -->

<aside class="notes"><p>Cada bloque convolucional recibe todo el conocimiento colectivo de las capas anteriores manteniendo el estado global de la red a la que se puede acceder</p>
</aside></script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

## Ventajas

- Mejor eficiencia de parámetros
<small>No es necesario volver a aprender los mapas de características redundantes</small>

- Fortalecen la propagación de características

- Fomentan la reutilización de características
</script></section></section><section ><section data-markdown><script type="text/template">
## Prototypical Networks
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

## Prototypical Networks

Modelo de meta-learning para el problema de la clasificación de pocos "disparos".

<aside class="notes"><p>Few shot learning generalmente se mide por su rendimiento en tareas de clasificación n-shot y k-way.</p>
</aside></script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

### Algoritmo

![](static/protonets.png)
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

### Algoritmo

- Un modelo es dado de una muestra de consulta que pertenece a una clase nueva nunca antes vista.

- Se proporciona un conjunto de soporte que consta de n ejemplos, cada uno de k diferentes clases no vistas.

- Determinar a qué clase de conjunto de soporte pertenecen las muestras de consulta.
</script></section><section data-markdown><script type="text/template">
También útiles para entrenar pequeños conjuntos de datos donde se conocen todas las clases.

</script></section></section><section ><section data-markdown><script type="text/template">
## Reconocimiento de forma de mano
</script></section><section data-markdown><script type="text/template">
### Conjuntos de datos

![datasets](static/datasets.png)
<!-- .element: style="width: 75%" -->
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left;" -->

## LSA16
<small style="margin-top: -25px;"><em>
Lenguaje de Señas Argentino (LSA)
</em></small>      

<div class="row" style="font-size: 2.53rem;">
    <div class="col-6">
        ![lsa16](static/lsa16.png)
        <!-- .element: style="width: 75%;" -->
    </div>
    <div class="col-6">

        <ul>
           <li>16 formas de manos</li>
           <li>800 imagenes</li>
           <li>32x32</li>
           <li>rgb</li>
        </ul>
    </div>
</div>
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left;" -->

## RWTH
<small style="margin-top: -25px;"><em>
Intérpretes de lenguaje de señas en la estación de televisión pública alemana PHOENIX
</em></small>      

<div class="row" style="font-size: 2.53rem;">
    <div class="col-6">
        ![rwth](static/rwth.png)
        <!-- .element: style="width: 75%;" -->
    </div>
    <div class="col-6">
        <ul>
           <li>
                45 formas de manos
                <br>
                <small><em>Nos quedamos con aquellas que cuentan con más de 20 ejemplos.</em></small>
           </li>
           <li>3359 labelled + 17gb unlabeled</li>
           <li>132x92</li>
           <li>rgb</li>
        </ul>
    </div>
</div>
</script></section><section data-markdown><script type="text/template">
<!-- .slide: style="text-align: left;" -->

## CIARP

<div class="row" style="font-size: 2.53rem;">
    <div class="col-6">
        ![ciarp](static/ciarp.png)
        <!-- .element: style="width: 75%;" -->
    </div>
    <div class="col-6">
        <ul>
           <li>10 formas de manos</li>
           <li>6000 imagenes</li>
           <li>38x38</li>
           <li>Cámara de único color</li>
           <li>Etiquetado manual</li>
        </ul>
    </div>
</div>
</script></section><section data-markdown><script type="text/template">
## Pasos
</script></section><section data-markdown><script type="text/template">
### Data Augmentation

- Flip
- Rotación (10 a 30 grados)
- Redimensionar (10% o 20% en ancho y alto)
</script></section><section data-markdown><script type="text/template">
### Resultados

![results](static/results.png)
</script></section><section data-markdown><script type="text/template">
#### Modelos entrenados con diferentes tamaños de muestra

<div class="row" style="width: 125%; transform: translate(-10%, -0%);">
    <div class="col-4">
        ![results](static/results/lsa16.png)
        <strong>LSA16</strong>
    </div>
    <div class="col-4">
        ![results](static/results/rwth.png)
        <strong>RWTH</strong>
    </div>
    <div class="col-4">
        ![results](static/results/ciarp.png)
        <strong>CIARP</strong>
    </div>
</div>
</script></section></section><section  data-markdown><script type="text/template">
<!-- .slide: style="text-align: left" -->

### ¿Qué sigue?

- Comparar con otros datasets
<small>Entender mejor la relación entre los modelos y la complejidad del conjunto de datos</small>

- Comparar modelos previamente entrenados

- Investigar métodos que aprovechen datos no etiquetados.

- Combinar conjuntos de datos para aumentar la muestra.

- Data augmentation.
</script></section></div>
    </div>

    <script src="./lib/js/head.min.js"></script>
    <script src="./js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './lib/js/classList.js', condition: function() { return !document.body.classList; } },
        { src: './plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './plugin/zoom-js/zoom.js', async: true },
        { src: './plugin/notes/notes.js', async: true },
        { src: './plugin/math/math.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
